{
 "cells": [
  {
   "source": [
    "## N-Gram 語言模型\n",
    "\n",
    "N-Gram 語言模型是基於統計的語言模型算法，\n",
    "基於馬可夫假設將文本中的內容取最靠近的 N 個字當作條件機率計算的先驗條件，\n",
    "形成長度是 N 的字詞片段序列。每個字詞片段及稱為gram。\n",
    "\n",
    "1. 了解如何實作與使用 N-gram 模型\n",
    "2. 透過實作更加了解 N-gram 模型\n",
    "3. 使用NLTK建構 N-gram 模型"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 了解如何實作與使用N_gram模型\n",
    "N_gram模型是基於統計的基礎語言模型，在這次的課程中，我們會學習到如何使用pyhon來實作N_gram的語言模型。\n",
    "課程中的範例我們會使用[傲慢與偏見的英文文本](https://www.gutenberg.org/ebooks/1342)來進行。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 進行資料清洗\n",
    "在搭建語言模型前，需要將文本資料根據需求進行清洗，像是去除標點符號、將文字正規化(大小寫轉換)等等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "717572 , ﻿The Project Gutenberg EBook of Pride and Prejudic ...\n"
     ]
    }
   ],
   "source": [
    "#讀取文本資料\n",
    "\n",
    "import urllib, requests\n",
    "\n",
    "books = {'Pride and Prejudice': '1342',\n",
    "         'Huckleberry Fin': '76',\n",
    "         'Sherlock Holmes': '1661'}\n",
    "\n",
    "book = books['Pride and Prejudice']\n",
    "\n",
    "\n",
    "url_template = f'https://www.gutenberg.org/cache/epub/{book}/pg{book}.txt'\n",
    "response = requests.get(url_template)\n",
    "txt = response.text\n",
    "\n",
    "#檢查文本\n",
    "print(len(txt), ',', txt[:50] , '...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在開始搭建N-gram模型之前，先對文本進行清洗\n",
    "(移除非英文與數字字元，且將所有的字轉為小寫)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "125897\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "words = re.split('[^A-Za-z]+', txt.lower())\n",
    "words = [x for x in words if x != ''] # 移除空字串\n",
    "\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 建立詞頻字典\n",
    "N-gram模型所需的機率需要使用詞頻來做計算，因此先對文本進行詞頻計算。\n",
    "這裡使用Unigram與Bigram進行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[('the', 4507), ('to', 4243), ('of', 3730), ('and', 3658), ('her', 2225), ('i', 2070), ('a', 2012), ('in', 1937), ('was', 1847), ('she', 1710)]\n"
     ]
    }
   ],
   "source": [
    "# 建立詞頻字典unigram\n",
    "unigram_frequecy = dict()\n",
    "\n",
    "# 計算詞頻\n",
    "for word in words:\n",
    "    unigram_frequecy[word] = unigram_frequecy.get(word, 0) + 1\n",
    "    # print(unigram_frequecy)\n",
    "    # print(\"==============\")\n",
    "    # print(unigram_frequecy[word])\n",
    "    # print(\"==============\")\n",
    "    # print(unigram_frequecy.get(word, 0) )\n",
    "    # print(\"==============\")\n",
    "\n",
    "# 根據詞頻排序, 並轉換為(word, count)格式\n",
    "unigram_frequecy = sorted(unigram_frequecy.items(), key=lambda word_count: word_count[1], reverse=True)\n",
    "\n",
    "# 查看詞頻前10的字詞\n",
    "print(unigram_frequecy[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bigram\n",
    "bigram_frequency = dict()\n",
    "\n",
    "for i in range(0, len(words)-1):\n",
    "    bigram_frequency[tuple(words[i:i+2])] = bigram_frequency.get(tuple(words[i:i+2]), 0) + 1\n",
    "    \n",
    "# 根據詞頻排序, 並轉換為(word, count)格式\n",
    "bigram_frequency = sorted(bigram_frequency.items(), key=lambda words_count: words_count[1], reverse=True)\n",
    "\n",
    "# 查看詞頻前10的字詞\n",
    "bigram_frequency[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 搭建N-gram模型\n",
    "接下來可以隨機選取一個初始的單字，再利用建立好的N-gram模型(這裡我們採用bigram)，\n",
    "來預測機率最高的10個字組成的句子，若沒有找到接續的詞，則中斷預測。\n",
    "\n",
    "這裡我們建立的Bigram詞頻表為(word_pairs, count)格式，其中word_pairs為(word_1, word_2)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立 Bigram 模型\n",
    "def do_bigram_prediction(bigram_freq, start_word, num_words):\n",
    "    #定義起始字\n",
    "    pred_words = [start_word]\n",
    "    word = start_word\n",
    "    for i in range(num_words):\n",
    "        # 找尋下一個字\n",
    "        word = pred_words[i]\n",
    "        word = next((word_pairs[1] for (word_pairs, count) in bigram_freq if word_pairs[0] == word), None)\n",
    "        \n",
    "        if not word:\n",
    "            break\n",
    "        else:\n",
    "            pred_words.append(word)\n",
    "    return pred_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用建立好的 Bigram 模型進行預測\n",
    "import random\n",
    "\n",
    "# 隨機選取起始字\n",
    "start_word = random.choice(words)\n",
    "print(f'初始字: {start_word}')\n",
    "\n",
    "# 使用選取的起始字預測接下來的字詞(共10個字)\n",
    "pred_words = do_bigram_prediction(bigram_frequency, start_word, 10)\n",
    "\n",
    "# 顯示預測結果\n",
    "print(f\"預測句子: {' '.join(pred_words)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加入選取權重\n",
    "依照上述的模型，會產生的問題是，如果開頭的字都相同(ex: of)，則後續的預測句子都一定會相同，\n",
    "為了修正這樣的問題，我們可以利用N-gram詞頻中的頻率當作權重，增加隨機性，以確保後續預測的句子都不一定會相同。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 權重選取\n",
    "def weighted_choice(choices):\n",
    "   total = sum(w for c, w in choices)\n",
    "   r = random.uniform(0, total)\n",
    "   upto = 0\n",
    "   for c, w in choices:\n",
    "      if upto + w > r:\n",
    "         return c\n",
    "      upto += w\n",
    "    \n",
    "def do_bigram_weighted_prediction(bigram_freq, start_word, num_words):\n",
    "    pred_words = [start_word]\n",
    "    # word = start_word\n",
    "    for i in range(num_words):\n",
    "        # 選取所有符合條件的2gram\n",
    "        word = pred_words[i]\n",
    "        words_candidates = [word_pairs_count for word_pairs_count in bigram_freq if word_pairs_count[0][0] == word]\n",
    "        if not words_candidates:\n",
    "            break\n",
    "        else:\n",
    "            #根據加權機率選取可能的字詞\n",
    "            pred_words.append(weighted_choice(words_candidates)[1])\n",
    "            \n",
    "    return pred_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_word = 'of'\n",
    "print(f'初始字: {start_word}')\n",
    "\n",
    "pred_words = do_bigram_weighted_prediction(bigram_frequency, start_word, 10)\n",
    "print(f\"預測句子: {' '.join(pred_words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_word = 'of'\n",
    "print(f'初始字: {start_word}')\n",
    "\n",
    "pred_words = do_bigram_weighted_prediction(bigram_frequency, start_word, 10)\n",
    "print(f\"預測句子: {' '.join(pred_words)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以發現，相同的起始字，也可能得到不同的預測結果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下來我們可以根據上述的搭建方式，來建立泛化的N-gram模型(可自由選定N值)，根據N值的不同可以得到如Unigram(N=1)，Bigram(N=2)，Trigram(N=3)等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateNgram(N):\n",
    "    gram_frequency = dict()\n",
    "    \n",
    "    # 避免N值過大，導致記憶體崩潰問題(先設定N < 100)\n",
    "    assert N > 0 and N < 100\n",
    "    \n",
    "    # 建立N-gram的頻率字典\n",
    "    for i in range(len(words)-(N-1)):\n",
    "        gram_frequency[tuple(words[i:i+N])] = gram_frequency.get(tuple(words[i:i+N]), 0) + 1\n",
    "\n",
    "    # 根據詞頻排序, 並轉換為(word, count)格式\n",
    "    gram_frequency = sorted(gram_frequency.items(), key=lambda words_count: words_count[1], reverse=True)\n",
    "    \n",
    "    return gram_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立Trigram\n",
    "trigram_frequency = generateNgram(3)\n",
    "\n",
    "# 查看詞頻前10的字詞\n",
    "trigram_frequency[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#建立N-gram預測function\n",
    "def do_ngram_weighted_prediction(gram_freq, start_word, num_words):\n",
    "    pred_words = [start_word]\n",
    "    # word = start_word\n",
    "    for i in range(num_words):\n",
    "        # 選取所有符合條件\n",
    "        word = pred_words[i]\n",
    "        words_candidates = [word_pairs_count for word_pairs_count in gram_freq if word_pairs_count[0][0] == word]\n",
    "        \n",
    "        if not words_candidates:\n",
    "            break\n",
    "        else:\n",
    "            #根據加權機率選取可能的字詞\n",
    "            pred_words.append(weighted_choice(words_candidates)[1])\n",
    "            \n",
    "    return pred_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_word = 'of'\n",
    "print(f'初始字: {start_word}')\n",
    "\n",
    "pred_words = do_ngram_weighted_prediction(trigram_frequency, start_word, 10)\n",
    "print(f\"預測推薦字詞: {' '.join(pred_words)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用NLTK套件搭建N-gram模型\n",
    "\n",
    "NLTK(Natural Language Toolkit)使一個python的自然語言處理套件，內含許多應用，這裡我們會使用NLTK來建立N-gram模型。\n",
    "\n",
    "首先需要進行套件安裝，只需要執行\n",
    "\n",
    "```bash\n",
    "pip install nltk\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from nltk import ngrams\n",
    "\n",
    "#使用NLTK API搭建Bigram\n",
    "bigram_frequency = ngrams(words, n=2)\n",
    "\n",
    "#使用collectins套件計算詞頻\n",
    "bigram_frequency = collections.Counter(bigram_frequency)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import collections\n",
    "from nltk import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#讀取文本資料\n",
    "\n",
    "import urllib, requests\n",
    "\n",
    "books = {'Pride and Prejudice': '1342',\n",
    "         'Huckleberry Fin': '76',\n",
    "         'Sherlock Holmes': '1661'}\n",
    "\n",
    "book = books['Pride and Prejudice']\n",
    "\n",
    "\n",
    "url_template = f'https://www.gutenberg.org/cache/epub/{book}/pg{book}.txt'\n",
    "response = requests.get(url_template)\n",
    "txt = response.text\n",
    "\n",
    "#檢查文本\n",
    "print(len(txt), ',', txt[:50] , '...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = re.split('[^A-Za-z]+', txt.lower())\n",
    "words = [x for x in words if x != ''] # 移除空字串\n",
    "\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立bigram詞頻 (注意這裡回傳的是generator)\n",
    "bigram_frequency = ngrams(words, n=2) \n",
    "\n",
    "# 計算詞頻\n",
    "bigram_frequency = collections.Counter(bigram_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.6 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "45c0168258897a5e08f6cb28b901f835a2644207f233a468a74df6ac8ef93992"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}